{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install rouge_score evaluate pyngrok ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:06:37.737885Z","iopub.execute_input":"2025-02-10T18:06:37.738192Z","iopub.status.idle":"2025-02-10T18:06:46.639291Z","shell.execute_reply.started":"2025-02-10T18:06:37.738153Z","shell.execute_reply":"2025-02-10T18:06:46.638003Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting pyngrok\n  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=07b8035b092cf1dd7c1df7171afe7e3b5f4d3a12a8af68339aee5182a9378cc2\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: pyngrok, rouge_score, evaluate\nSuccessfully installed evaluate-0.4.3 pyngrok-7.2.3 rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!curl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\\n\t| sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null \\\n\t&& echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" \\\n\t| sudo tee /etc/apt/sources.list.d/ngrok.list \\\n\t&& sudo apt update \\\n\t&& sudo apt install ngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:06:46.640545Z","iopub.execute_input":"2025-02-10T18:06:46.640933Z","iopub.status.idle":"2025-02-10T18:07:01.409456Z","shell.execute_reply.started":"2025-02-10T18:06:46.640875Z","shell.execute_reply":"2025-02-10T18:07:01.407962Z"}},"outputs":[{"name":"stdout","text":"deb https://ngrok-agent.s3.amazonaws.com buster main\nGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:2 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]          \u001b[0m\u001b[33m\nGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \u001b[0m\u001b[33m\nHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                         \u001b[0m\nGet:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \u001b[0m\nGet:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [66.7 kB]\u001b[33m\nGet:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \u001b[0m\u001b[33m\u001b[33m\nGet:9 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [7,389 B]\u001b[0m\u001b[33m\nGet:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,309 kB]\nGet:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]m\u001b[33m\u001b[33m\nGet:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\nGet:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,663 kB] \u001b[0m\nGet:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nGet:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [45.2 kB]\nHit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \u001b[0m\u001b[33m\u001b[33m\nGet:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,230 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,748 kB]\nGet:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,606 kB]m\nGet:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,653 kB][0m\u001b[33m\u001b[33m\nGet:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\nGet:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,606 kB]3m\nGet:23 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [57.8 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,910 kB][33m\u001b[33m\u001b[33m\u001b[33m\nGet:25 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,523 kB]\nGet:26 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nFetched 29.0 MB in 3s (8,851 kB/s)33m                         \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\n124 packages can be upgraded. Run 'apt list --upgradable' to see them.\n\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  ngrok\n0 upgraded, 1 newly installed, 0 to remove and 124 not upgraded.\nNeed to get 9,898 kB of archives.\nAfter this operation, 0 B of additional disk space will be used.\nGet:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.19.1 [9,898 kB]\nFetched 9,898 kB in 0s (29.5 MB/s)0m\u001b[33m\ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\ndebconf: falling back to frontend: Readline\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package ngrok.\n(Reading database ... 127400 files and directories currently installed.)\nPreparing to unpack .../ngrok_3.19.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking ngrok (3.19.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up ngrok (3.19.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ngrok config add-authtoken 2sLV4iA8j8iRMAjLywMaB7vLkiD_2nEwWQKE4rAYsFw59Mg4v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:07:01.410747Z","iopub.execute_input":"2025-02-10T18:07:01.411028Z","iopub.status.idle":"2025-02-10T18:07:01.576514Z","shell.execute_reply.started":"2025-02-10T18:07:01.411002Z","shell.execute_reply":"2025-02-10T18:07:01.574629Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from pyngrok import ngrok\nfrom tensorboard import program\n\nlog_dir = '/kaggle/working/logs'  \ntb = program.TensorBoard()\ntb.configure(argv=[None, '--logdir', log_dir])\n\nurl = tb.launch()\n\npublic_url = ngrok.connect(6006)  \npublic_url","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:07:01.578785Z","iopub.execute_input":"2025-02-10T18:07:01.579107Z","iopub.status.idle":"2025-02-10T18:07:06.992587Z","shell.execute_reply.started":"2025-02-10T18:07:01.579078Z","shell.execute_reply":"2025-02-10T18:07:06.991525Z"}},"outputs":[{"name":"stderr","text":"\nNOTE: Using experimental fast data loading logic. To disable, pass\n    \"--load_fast=false\" and report issues on GitHub. More details:\n    https://github.com/tensorflow/tensorboard/issues/4784\n\n","output_type":"stream"},{"name":"stdout","text":"                                                                                                    \r","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<NgrokTunnel: \"https://c078-34-105-107-239.ngrok-free.app\" -> \"http://localhost:6006\">"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset, load_from_disk, DatasetDict\n\noriginal_dataset = load_dataset(\"scientific_papers\", \"arxiv\", trust_remote_code=True) \noriginal_dataset = original_dataset.remove_columns(\"section_names\")\noriginal_dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:46:04.661282Z","iopub.execute_input":"2025-02-10T10:46:04.661817Z","iopub.status.idle":"2025-02-10T10:51:18.485784Z","shell.execute_reply.started":"2025-02-10T10:46:04.661760Z","shell.execute_reply":"2025-02-10T10:51:18.484950Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5c81102a23a4120a9ec599fa442fe5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scientific_papers.py:   0%|          | 0.00/5.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d8ba97206b485bb00d2c510334bae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.62G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d9b7ff0dd140ddb94c7c32dde62120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/880M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97e02ac756a649569e61db8ae02fd460"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/203037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6e2bb214e5e475295e1c3668e86c346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/6436 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f626dc7388e46dd9028c07e9df0467b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff51c0736f4f435da96e26c5a4cbb136"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'abstract'],\n        num_rows: 203037\n    })\n    validation: Dataset({\n        features: ['article', 'abstract'],\n        num_rows: 6436\n    })\n    test: Dataset({\n        features: ['article', 'abstract'],\n        num_rows: 6440\n    })\n})"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"train_slice = int(len(original_dataset['train']) * 0.05)\ntrain_dataset = original_dataset['train'].select(range(train_slice))\ntrain_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:51:18.486962Z","iopub.execute_input":"2025-02-10T10:51:18.487277Z","iopub.status.idle":"2025-02-10T10:51:18.494948Z","shell.execute_reply.started":"2025-02-10T10:51:18.487252Z","shell.execute_reply":"2025-02-10T10:51:18.494271Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['article', 'abstract'],\n    num_rows: 10151\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import re\n\ndef replace_semicolon(text, threshold=10):\n    new_text = \"\"\n    for subset in re.split(';', text):\n        subset = subset.strip() \n        if len(subset.split()) > threshold:\n            new_text += \". \" + subset[0].upper() + subset[1:]\n        else:\n            new_text += \", \" + subset\n\n    return new_text\n\nPAREN_re = re.compile('\\([^(]+\\ [^\\(]+\\)')\nBAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('\"#%&\\*\\+/<=>@[\\]^{|}~_$'), re.UNICODE)\nBULLET_RE = re.compile('\\n[\\ \\t]*`*\\([a-zA-Z0-9]*\\)')\nDASH_RE = re.compile('--+')\nWHITESPACE_RE = re.compile('\\s+')\nEMPTY_SENT_RE = re.compile('[,\\.]\\ *[\\.,]')\nFIX_START_RE = re.compile('^[^A-Za-z]*')\nFIX_PERIOD = re.compile('\\.([A-Za-z])')\nREF_RE = re.compile(r\"@\\w+|see e\\.g\\..*?(\\.|$)|refer to.*?(\\.|$)\")\nFIX_SPACES_AROUND_PUNCT = re.compile(r'\\s+([.,!?;:])')\nFIX_PARENTHESES = re.compile(r'\\(\\s*([^)]*?)\\s*\\)')\n\ndef clean_text(text):\n    text = REF_RE.sub('', text)\n    text = PAREN_re.sub('', text)\n    text = BULLET_RE.sub(' ', text)\n    text = text.replace('&lt;all&gt;', '')\n    text = BAD_PUNCT_RE.sub('', text)\n    text = DASH_RE.sub(' ', text)\n    text = WHITESPACE_RE.sub(' ', text)\n    text = EMPTY_SENT_RE.sub('.', text)\n    text = replace_semicolon(text)\n    text = FIX_START_RE.sub('', text)\n    text = FIX_PERIOD.sub(\". \\g<1>\", text)\n    text = text.replace('``', '\"')\n    text = text.replace('\\'\\'', '\"')\n    text = FIX_SPACES_AROUND_PUNCT.sub(r'\\1', text)\n    text = FIX_PARENTHESES.sub(r'(\\1)', text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:51:18.496188Z","iopub.execute_input":"2025-02-10T10:51:18.496433Z","iopub.status.idle":"2025-02-10T10:51:19.416508Z","shell.execute_reply.started":"2025-02-10T10:51:18.496413Z","shell.execute_reply":"2025-02-10T10:51:19.415545Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os \n\ndef clean_text_batch(batch):\n    batch[\"article\"] = [clean_text(text) for text in batch[\"article\"]]\n    batch[\"abstract\"] = [clean_text(text) for text in batch[\"abstract\"]]\n    return batch\n\nNUM_CPU = os.cpu_count()\n\ncleaned_dataset = train_dataset.map(clean_text_batch, batched=True, num_proc=NUM_CPU)\ncleaned_dataset.save_to_disk(\"train_dataset_cleaned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:52:05.982211Z","iopub.execute_input":"2025-02-10T10:52:05.982558Z","iopub.status.idle":"2025-02-10T10:53:31.069090Z","shell.execute_reply.started":"2025-02-10T10:52:05.982529Z","shell.execute_reply":"2025-02-10T10:53:31.068301Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10151 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8504e7502a564c9c88a729b04e7b57de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/10151 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a185db7672c646ea88abddc0e3aab645"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"text = train_dataset['article'][0][:500]\n\nprint(\"Text before cleaning: \\n\")\nprint(text)\nprint(\"\\n#####################\\n\")\nprint(\"Text after cleaning: \\n\")\nprint(clean_text(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:53:31.070611Z","iopub.execute_input":"2025-02-10T10:53:31.070962Z","iopub.status.idle":"2025-02-10T10:53:31.467896Z","shell.execute_reply.started":"2025-02-10T10:53:31.070934Z","shell.execute_reply":"2025-02-10T10:53:31.466861Z"}},"outputs":[{"name":"stdout","text":"Text before cleaning: \n\nadditive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models .\nit is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models .\n\n#####################\n\nText after cleaning: \n\nAdditive models provide an important family of models for semiparametric regression or classification. some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models. it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n)\n\nfrom peft import (\n    LoraConfig, \n    get_peft_model, \n    TaskType\n)\n\nimport nltk\nfrom datasets import load_dataset, load_from_disk, DatasetDict\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:09:15.297678Z","iopub.execute_input":"2025-02-10T18:09:15.298043Z","iopub.status.idle":"2025-02-10T18:09:15.304240Z","shell.execute_reply.started":"2025-02-10T18:09:15.298018Z","shell.execute_reply":"2025-02-10T18:09:15.302980Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_dataset = load_from_disk(\"train_dataset_cleaned\")\ntrain_data_txt, validation_data_txt = train_dataset.train_test_split(test_size=0.1).values()\ntrain_data_txt, validation_data_txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:09:45.767182Z","iopub.execute_input":"2025-02-10T18:09:45.767568Z","iopub.status.idle":"2025-02-10T18:09:45.948833Z","shell.execute_reply.started":"2025-02-10T18:09:45.767539Z","shell.execute_reply":"2025-02-10T18:09:45.947815Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['article', 'abstract'],\n     num_rows: 9135\n }),\n Dataset({\n     features: ['article', 'abstract'],\n     num_rows: 1016\n }))"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"base_model_name = 'facebook/bart-large-cnn'\n\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n\n# tokenization\n# encoder_max_length = 256\n# decoder_max_length = 64\n\nencoder_max_length = 1024\ndecoder_max_length = 256\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name, max_length=encoder_max_length, truncation=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:08:10.006048Z","iopub.execute_input":"2025-02-10T18:08:10.007544Z","iopub.status.idle":"2025-02-10T18:08:23.144697Z","shell.execute_reply.started":"2025-02-10T18:08:10.007511Z","shell.execute_reply":"2025-02-10T18:08:23.142908Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90f033e070b24f17aa955b55a43caa53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d095c9e31ff74cb49baa963ecf20a421"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"435a5fbd450e4e84a6ef81c8e3c85531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f5f780c4fa412487efe27f4c300ced"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dba662b86d4643fda8fd862dd620ebec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f44364fa2aed43eabb39b77fa12e24ef"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,  \n    lora_alpha=32,  \n    lora_dropout=0.1,  \n    bias=\"none\",  \n    task_type=TaskType.SEQ_2_SEQ_LM \n)\n\nbase_model_peft = get_peft_model(base_model, lora_config)\nbase_model_peft.print_trainable_parameters() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:54:03.511188Z","iopub.execute_input":"2025-02-10T10:54:03.511539Z","iopub.status.idle":"2025-02-10T10:54:03.630137Z","shell.execute_reply.started":"2025-02-10T10:54:03.511507Z","shell.execute_reply":"2025-02-10T10:54:03.629401Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,359,296 || all params: 408,649,728 || trainable%: 0.5773\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n    source, target = batch[\"article\"], batch[\"abstract\"]\n    source_tokenized = tokenizer(\n        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n    )\n    target_tokenized = tokenizer(\n        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n    )\n\n    batch = {k: v for k, v in source_tokenized.items()}\n    batch[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in l]\n        for l in target_tokenized[\"input_ids\"]\n    ]\n    return batch\n\ntrain_data = train_data_txt.map(\n    lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=train_data_txt.column_names,\n    num_proc=NUM_CPU,\n)\n\nvalidation_data = validation_data_txt.map(\n    lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=validation_data_txt.column_names,\n    num_proc=NUM_CPU,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:55:17.614278Z","iopub.execute_input":"2025-02-10T10:55:17.614602Z","iopub.status.idle":"2025-02-10T10:56:44.623562Z","shell.execute_reply.started":"2025-02-10T10:55:17.614578Z","shell.execute_reply":"2025-02-10T10:56:44.622580Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/9135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2738271d17284f0488f4d39b1bd2e936"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1016 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd98960bcf044d09eda2918c79d8c1a"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"folder_name = \"preprocessed_dataset\"\n# train_data.save_to_disk(f\"{folder_name}/train\")\n# validation_data.save_to_disk(f\"{folder_name}/val\")\n\ntrain_data = load_from_disk(f\"{folder_name}/train\")\nvalidation_data = load_from_disk(f\"{folder_name}/val\")\n\ntrain_data, validation_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:09:19.718366Z","iopub.execute_input":"2025-02-10T18:09:19.718760Z","iopub.status.idle":"2025-02-10T18:09:20.043073Z","shell.execute_reply.started":"2025-02-10T18:09:19.718732Z","shell.execute_reply":"2025-02-10T18:09:20.041913Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['input_ids', 'attention_mask', 'labels'],\n     num_rows: 9135\n }),\n Dataset({\n     features: ['input_ids', 'attention_mask', 'labels'],\n     num_rows: 1016\n }))"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"train_data['input_ids'][0][:10], train_data['attention_mask'][0][:10], train_data['labels'][0][:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:57:08.207023Z","iopub.execute_input":"2025-02-10T10:57:08.207369Z","iopub.status.idle":"2025-02-10T10:57:17.960483Z","shell.execute_reply.started":"2025-02-10T10:57:08.207346Z","shell.execute_reply":"2025-02-10T10:57:17.959681Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"([0, 250, 467, 9, 33310, 23140, 19, 10, 1537, 22039],\n [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n [0, 4688, 12547, 9, 7268, 1848, 5536, 33310, 16, 10])"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import evaluate\n\nnltk.download(\"punkt\", quiet=True)\n\nmetric = evaluate.load(\"rouge\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(\n        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n    )\n    result = {key: value * 100 for key, value in result.items()}\n\n    prediction_lens = [\n        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n    ]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:08:23.147435Z","iopub.execute_input":"2025-02-10T18:08:23.147885Z","iopub.status.idle":"2025-02-10T18:08:24.773420Z","shell.execute_reply.started":"2025-02-10T18:08:23.147848Z","shell.execute_reply":"2025-02-10T18:08:24.771947Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f10896b62134fd0ae4179b053e43a89"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Custom function to prevent errors in Hugging Face code\n\ndef _custom_pad_tensors_to_max_len(self, tensor, max_length):\n        if self.processing_class is not None and hasattr(self.processing_class, \"pad_token_id\"):\n            # If PAD token is not defined at least EOS token has to be defined\n            pad_token_id = (\n                self.processing_class.pad_token_id if self.processing_class.pad_token_id is not None else self.processing_class.eos_token_id\n            )\n        else:\n            if self.model.config.pad_token_id is not None:\n                pad_token_id = self.model.config.pad_token_id\n            else:\n                raise ValueError(\"Pad_token_id must be set in the configuration of the model, in order to pad tensors\")\n\n        padded_tensor = pad_token_id * torch.ones(\n            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n        )\n        padded_tensor[:, : tensor.shape[-1]] = tensor\n        return padded_tensor\n\nSeq2SeqTrainer._pad_tensors_to_max_len = _custom_pad_tensors_to_max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:08:39.243258Z","iopub.execute_input":"2025-02-10T18:08:39.245749Z","iopub.status.idle":"2025-02-10T18:08:39.285500Z","shell.execute_reply.started":"2025-02-10T18:08:39.245665Z","shell.execute_reply":"2025-02-10T18:08:39.283707Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./lora_model\",\n    num_train_epochs=6,  \n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.1,\n    label_smoothing_factor=0.1,\n    predict_with_generate=True,\n    logging_dir=\"logs\",\n    logging_steps=50,\n    save_total_limit=3,\n    report_to=[\"tensorboard\"]\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=base_model_peft)\n\ntrainer = Seq2SeqTrainer(\n    model=base_model_peft,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n    eval_dataset=validation_data, \n    processing_class=tokenizer,  \n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:02:29.868177Z","iopub.execute_input":"2025-02-10T11:02:29.868498Z","iopub.status.idle":"2025-02-10T11:02:30.976820Z","shell.execute_reply.started":"2025-02-10T11:02:29.868475Z","shell.execute_reply":"2025-02-10T11:02:30.976035Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:02:33.987928Z","iopub.execute_input":"2025-02-10T11:02:33.988216Z","iopub.status.idle":"2025-02-10T11:21:20.757942Z","shell.execute_reply.started":"2025-02-10T11:02:33.988193Z","shell.execute_reply":"2025-02-10T11:21:20.757070Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='128' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [64/64 4:35:14]\n    </div>\n    "},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 4.447519302368164,\n 'eval_rouge1': 30.547,\n 'eval_rouge2': 8.903,\n 'eval_rougeL': 18.0604,\n 'eval_rougeLsum': 27.0716,\n 'eval_gen_len': 80.6555,\n 'eval_runtime': 1126.7567,\n 'eval_samples_per_second': 0.902,\n 'eval_steps_per_second': 0.057}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T11:30:00.658049Z","iopub.execute_input":"2025-02-10T11:30:00.658382Z","iopub.status.idle":"2025-02-10T15:14:39.376878Z","shell.execute_reply.started":"2025-02-10T11:30:00.658360Z","shell.execute_reply":"2025-02-10T15:14:39.375959Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3426' max='3426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3426/3426 3:44:33, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>4.550400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.469200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>4.415200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.349900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>4.270800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>4.249900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>4.244500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>4.220800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>4.193700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>4.163900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>4.150400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>4.158200</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>4.147200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>4.139500</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>4.178000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>4.143800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>4.148000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>4.177400</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>4.146500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>4.119800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>4.104900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>4.120500</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>4.175100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>4.129200</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>4.137800</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>4.108300</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>4.137200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>4.086000</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>4.145500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>4.077600</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>4.082700</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>4.110900</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>4.099500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>4.115000</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>4.065600</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>4.087100</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>4.128600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>4.106000</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>4.093000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>4.079800</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>4.122400</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>4.089900</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>4.089000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>4.095500</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>4.089500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>4.070300</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>4.069700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>4.096800</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>4.105600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>4.075800</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>4.078300</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>4.088600</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>4.105800</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>4.113300</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>4.060700</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>4.064000</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>4.059000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>4.061000</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>4.081200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>4.065000</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>4.077800</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>4.094300</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>4.077900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>4.115500</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>4.092200</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>4.075800</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>4.060000</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>4.063700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3426, training_loss=4.1341824083389485, metrics={'train_runtime': 13478.032, 'train_samples_per_second': 4.067, 'train_steps_per_second': 0.254, 'total_flos': 1.1957350239830016e+17, 'train_loss': 4.1341824083389485, 'epoch': 6.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:14:39.378075Z","iopub.execute_input":"2025-02-10T15:14:39.378315Z","iopub.status.idle":"2025-02-10T15:38:26.128370Z","shell.execute_reply.started":"2025-02-10T15:14:39.378294Z","shell.execute_reply":"2025-02-10T15:38:26.127066Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 4.017017841339111,\n 'eval_rouge1': 42.4221,\n 'eval_rouge2': 15.4355,\n 'eval_rougeL': 24.1791,\n 'eval_rougeLsum': 37.8461,\n 'eval_gen_len': 133.9715,\n 'eval_runtime': 1426.7385,\n 'eval_samples_per_second': 0.712,\n 'eval_steps_per_second': 0.045,\n 'epoch': 6.0}"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"model_path = '/kaggle/working/lora_model/checkpoint-3426'\ntuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:08:27.085277Z","iopub.execute_input":"2025-02-10T18:08:27.085956Z","iopub.status.idle":"2025-02-10T18:08:33.049483Z","shell.execute_reply.started":"2025-02-10T18:08:27.085883Z","shell.execute_reply":"2025-02-10T18:08:33.048225Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"tuned_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:08:33.051462Z","iopub.execute_input":"2025-02-10T18:08:33.051828Z","iopub.status.idle":"2025-02-10T18:08:33.068151Z","shell.execute_reply.started":"2025-02-10T18:08:33.051791Z","shell.execute_reply":"2025-02-10T18:08:33.067001Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=16, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=16, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict()\n            )\n            (q_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=16, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=16, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict()\n            )\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=16, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=16, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict()\n            )\n            (q_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=16, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=16, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict()\n            )\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=16, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=16, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict()\n            )\n            (q_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=16, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=16, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict()\n            )\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"article, abstract = validation_data_txt[0].values()\n\ninputs = tokenizer(article, padding=True, truncation=True, return_tensors=\"pt\", max_length=encoder_max_length).input_ids\nlabels = tokenizer(abstract, padding=True, truncation=True, return_tensors=\"pt\", max_length=encoder_max_length).input_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:09:55.003482Z","iopub.execute_input":"2025-02-10T18:09:55.003926Z","iopub.status.idle":"2025-02-10T18:09:55.077312Z","shell.execute_reply.started":"2025-02-10T18:09:55.003892Z","shell.execute_reply":"2025-02-10T18:09:55.076118Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ninputs = inputs.to(device)\nlabels = labels.to(device)\n\nbase_model = base_model.to(device)\ntuned_model = tuned_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:09:56.571761Z","iopub.execute_input":"2025-02-10T18:09:56.572190Z","iopub.status.idle":"2025-02-10T18:09:56.603724Z","shell.execute_reply.started":"2025-02-10T18:09:56.572160Z","shell.execute_reply":"2025-02-10T18:09:56.602586Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"base_outputs = base_model.generate(inputs, max_new_tokens=decoder_max_length, do_sample=False)\nbase_summary = tokenizer.decode(base_outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:09:57.800008Z","iopub.execute_input":"2025-02-10T18:09:57.800459Z","iopub.status.idle":"2025-02-10T18:10:19.537157Z","shell.execute_reply.started":"2025-02-10T18:09:57.800428Z","shell.execute_reply":"2025-02-10T18:10:19.535839Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"tuned_outputs = tuned_model.generate(inputs, max_new_tokens=decoder_max_length, do_sample=False)\ntuned_summary = tokenizer.decode(tuned_outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:10:19.538692Z","iopub.execute_input":"2025-02-10T18:10:19.539044Z","iopub.status.idle":"2025-02-10T18:11:03.387702Z","shell.execute_reply.started":"2025-02-10T18:10:19.539013Z","shell.execute_reply":"2025-02-10T18:11:03.386459Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"base_outputs = base_outputs.to(\"cpu\")\ntuned_outputs = tuned_outputs.to(\"cpu\")\nlabels = labels.to(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:11:03.389411Z","iopub.execute_input":"2025-02-10T18:11:03.389719Z","iopub.status.idle":"2025-02-10T18:11:03.394091Z","shell.execute_reply.started":"2025-02-10T18:11:03.389680Z","shell.execute_reply":"2025-02-10T18:11:03.393058Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"compute_metrics((base_outputs, labels)), compute_metrics((tuned_outputs, labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:11:03.395465Z","iopub.execute_input":"2025-02-10T18:11:03.395816Z","iopub.status.idle":"2025-02-10T18:11:03.805889Z","shell.execute_reply.started":"2025-02-10T18:11:03.395787Z","shell.execute_reply":"2025-02-10T18:11:03.804731Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"({'rouge1': 37.1585,\n  'rouge2': 2.2099,\n  'rougeL': 18.5792,\n  'rougeLsum': 31.694,\n  'gen_len': 82.0},\n {'rouge1': 54.7619,\n  'rouge2': 23.2,\n  'rougeL': 32.5397,\n  'rougeLsum': 48.4127,\n  'gen_len': 182.0})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"base_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:11:03.807027Z","iopub.execute_input":"2025-02-10T18:11:03.807407Z","iopub.status.idle":"2025-02-10T18:11:03.814190Z","shell.execute_reply.started":"2025-02-10T18:11:03.807363Z","shell.execute_reply":"2025-02-10T18:11:03.812915Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'The path - integral formulation of quantum mechanics is the foundation of many numerical methods that allow one to study with great accuracy the rich physics of interacting quantum systems. A number of important physical problems particularly in the fields of strongly correlated fermions and cold atoms can be fruitfully modeled by lattice hamiltonians. We propose a new method that generalizes and improves the approach of ref. ref.'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"tuned_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T18:11:03.815267Z","iopub.execute_input":"2025-02-10T18:11:03.815528Z","iopub.status.idle":"2025-02-10T18:11:03.834537Z","shell.execute_reply.started":"2025-02-10T18:11:03.815505Z","shell.execute_reply":"2025-02-10T18:11:03.833265Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'We present a generalization of the path - integral monte carlo (pimc) algorithm for lattice hamiltonians, based on continuous - time random walks, that is unaffected by time - step errors. we also introduce a worm - algorithm based method to calculate pure expectation values of arbitrary off - diagonal observables, which are generally out of the scope of existing lattice ground - state methods. the resulting algorithm naturally applies to fermions, using the fixed - node approximation, but a technique to improve systematically upon this approximation is proposed. our methodology is demonstrated by a few case studies on the one - dimensional heisenberg model and the two - dimensional fermion hubbard model. we show that the algorithm can be used to calculate the ground state energies of the fermionic hubbard models with a significantly better accuracy than that achieved by the fixed node approximation.'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Obsolete version\n\nimport concurrent.futures\nfrom datasets import Dataset, DatasetDict\nfrom transformers import BartTokenizer\nfrom tqdm import tqdm\nimport torch\n\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\ndef split_text_into_chunks(text, max_tokens=1024):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=False, padding=False)\n    input_ids = inputs[\"input_ids\"][0]\n\n    chunks = []\n    for i in range(0, len(input_ids), max_tokens):\n        chunk = input_ids[i : i + max_tokens]\n        chunks.append(tokenizer.decode(chunk, skip_special_tokens=True))\n    return chunks\n\ndef prepare_chunked_dataset(dataset, max_article_tokens=1024, max_summary_tokens=150):\n    chunked_articles = []\n    chunked_summaries = []\n    chunked_ids = []\n\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        article_chunks = list(tqdm(\n            executor.map(split_text_into_chunks, dataset[\"article\"], [max_article_tokens] * len(dataset[\"article\"])), \n            total=len(dataset[\"article\"]), desc=\"Chunking Articles\"\n        ))\n\n        summary_chunks = list(tqdm(\n            executor.map(split_text_into_chunks, dataset[\"abstract\"], [max_summary_tokens] * len(dataset[\"abstract\"])), \n            total=len(dataset[\"abstract\"]), desc=\"Chunking Summaries\"\n        ))\n\n    for idx, (art_chunks, sum_chunks) in enumerate(zip(article_chunks, summary_chunks)):\n        for art_chunk, sum_chunk in zip(art_chunks, sum_chunks):\n            chunked_articles.append(art_chunk)\n            chunked_summaries.append(sum_chunk)\n            chunked_ids.append(idx)  \n\n    return Dataset.from_dict({\"article\": chunked_articles, \"abstract\": chunked_summaries, \"example_id\": chunked_ids})\n\ndef tokenize_function(examples):\n    inputs = tokenizer(examples['article'], padding=\"max_length\", truncation=True, max_length=1024)\n    targets = tokenizer(examples['abstract'], padding=\"max_length\", truncation=True, max_length=150)\n    return {\n        \"input_ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long), \n        \"attention_mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long), \n        \"labels\": torch.tensor(targets[\"input_ids\"], dtype=torch.long),\n        \"example_id\": torch.tensor(examples[\"example_id\"], dtype=torch.long)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T22:19:47.291860Z","iopub.execute_input":"2025-01-29T22:19:47.292150Z","iopub.status.idle":"2025-01-29T22:19:54.882659Z","shell.execute_reply.started":"2025-01-29T22:19:47.292116Z","shell.execute_reply":"2025-01-29T22:19:54.881728Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff81fdb231ff4ff0b6c3a0618fa29ba7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5117fa79707a4926bbf949a1b558276d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ea954ffaab46d4858e539380294aab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4da042c4b0eb4607bbd201476a3ccdd4"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Obsolete version\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    predict_with_generate=True,\n    fp16=True,\n    logging_dir=\"./logs\",\n    remove_unused_columns=False,\n    report_to=['tensorboard']\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=base_model_peft)\n\ntrainer = Seq2SeqTrainer(\n    model=base_model_peft,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n    eval_dataset=validation_data,\n    processing_class=tokenizer,  \n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T20:33:53.624318Z","iopub.execute_input":"2025-01-29T20:33:53.624629Z","iopub.status.idle":"2025-01-29T21:56:02.150580Z","shell.execute_reply.started":"2025-01-29T20:33:53.624605Z","shell.execute_reply":"2025-01-29T21:56:02.149641Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1500/1500 1:22:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1 Fmeasure</th>\n      <th>Rouge2 Fmeasure</th>\n      <th>Rougel Fmeasure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.123500</td>\n      <td>3.117505</td>\n      <td>0.360189</td>\n      <td>0.103463</td>\n      <td>0.205191</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.093400</td>\n      <td>3.116834</td>\n      <td>0.358605</td>\n      <td>0.103081</td>\n      <td>0.206600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.070600</td>\n      <td>3.111456</td>\n      <td>0.362723</td>\n      <td>0.105114</td>\n      <td>0.207312</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1500, training_loss=3.095814697265625, metrics={'train_runtime': 4927.6089, 'train_samples_per_second': 2.435, 'train_steps_per_second': 0.304, 'total_flos': 2.6179201400832e+16, 'train_loss': 3.095814697265625, 'epoch': 3.0})"},"metadata":{}}],"execution_count":86}]}